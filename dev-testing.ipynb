{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/tqdm/autonotebook/__init__.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torecsys as trs\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get samples data from movielens as a example\n",
    "# _, movies_df, ratings_df, _ = trs.data.sampledata.load_ml_data(size=\"latest-small\")\n",
    "# movies_df[\"year\"] = movies_df.title.apply(lambda x: re.findall(r\"\\((\\d+)\\)\", x))\n",
    "# movies_df[\"year\"] = movies_df.year.apply(lambda x: int(x[0]) if len(x) > 0 else np.nan)\n",
    "# movies_df = pd.concat([\n",
    "#     movies_df, \n",
    "#     pd.get_dummies(movies_df.genres.apply(\n",
    "#         lambda x: x.split(\"|\")).apply(pd.Series).stack()).sum(level=0)\n",
    "# ], axis=1).drop([\"title\", \"genres\"], axis=1)\n",
    "# merged = pd.merge(ratings_df, movies_df, on=\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = torch.jit.trace(trs.inputs.base.EmbeddingDict([\"userId\", \"itemId\"], [\"single_index\", \"single_index\"], [100, 50], [16, 16]), {\"userId\": (torch.Tensor([[1]]).long()), \"itemId\": (torch.Tensor([[1]]).long())})\n",
    "emb_dict({\"userId\": (torch.Tensor([[1]]).long()), \"itemId\": (torch.Tensor([[1]]).long())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_emb = torch.jit.trace(trs.inputs.base.FieldAwareMultipleIndexEmbedding(16, [100, 10]), torch.Tensor([[1, 1]]).long())\n",
    "fa_emb(torch.Tensor([[1, 1]]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_emb = torch.jit.trace(trs.inputs.base.ListIndexEmbedding(16, 100, num_heads=4), torch.Tensor([[1, 4, 5]]).long())\n",
    "list_emb(torch.Tensor([[1, 4, 5]]).long())\n",
    "# try to fix if use trace\n",
    "# list_emb.show_attention(torch.Tensor([[1, 4, 5]]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_emb = torch.jit.trace(trs.inputs.base.MultipleIndexEmbedding(16, [100, 10]), torch.Tensor([[1, 1]]).long())\n",
    "mlp_emb(torch.Tensor([[1, 1]]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_emb = torch.jit.trace(trs.inputs.base.SequenceIndexEmbedding(16, 100), (torch.Tensor([[3, 1, 0], [4, 2, 5]]).long(), torch.Tensor([2, 3]).long()))\n",
    "sq_emb(torch.Tensor([[3, 1, 0], [4, 2, 5]]).long(), torch.Tensor([2, 3]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_emb = torch.jit.trace(trs.inputs.base.SingleIndexEmbedding(16, 100), torch.Tensor([[1]]).long())\n",
    "index_emb(torch.Tensor([[1]]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = torch.jit.trace(trs.inputs.base.StackedInputs([\"userId\", \"itemId\"], [\"single_index\", \"single_index\"], [100, 50], [16, 16]), ({\"userId\": (torch.Tensor([[1]]).long()), \"itemId\": (torch.Tensor([[1]]).long())}))\n",
    "stack({\"userId\": (torch.Tensor([[1]]).long()), \"itemId\": (torch.Tensor([[1]]).long())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = torch.jit.trace(trs.inputs.base.ValueInputs(5), torch.randn(size=(1, 5)))\n",
    "val(torch.randn(size=(1, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_inputs = torch.jit.trace(trs.inputs.base.ImageInputs(16, 3, [8, 8], [1, 1], [1, 1], [1, 1]), torch.randn(4, 3, 64, 64))\n",
    "img_inputs(torch.randn(4, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_img = torch.jit.trace(trs.inputs.base.PretrainedImageInputs(16, \"resnet18\"), torch.randn(4, 3, 64, 64))\n",
    "pre_img(torch.randn(4, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    (trs.inputs.base.SingleIndexEmbedding(16, 100), [\"userId\"]), \n",
    "    (trs.inputs.base.ValueInputs(5), [\"userVal\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = ConcatInputs(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 21])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat({\n",
    "    \"userId\": torch.randint(100, size=(8, 1)),\n",
    "    \"userVal\": torch.randn(size=(8, 5))\n",
    "}).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Field-aware Factorization Machine with trs.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 16\n",
    "num_fields = 2\n",
    "\n",
    "userIdSize = 100\n",
    "itemIdSize = 100\n",
    "\n",
    "# inititalize embedding fields\n",
    "feat_inputs_embedding = trs.inputs.base.MultipleIndexEmbedding(\n",
    "    1, [userIdSize, itemIdSize]\n",
    ")\n",
    "field_aware_embedding = trs.inputs.base.FieldAwareMultipleIndexEmbedding(\n",
    "    embed_size, [itemIdSize, itemIdSize]\n",
    ")\n",
    "\n",
    "# define schema of wrapper and initialize InputsWrapper\n",
    "schema = {\n",
    "    \"feat_inputs\"      : (feat_inputs_embedding, [\"userId\", \"movieId\"]),\n",
    "    \"field_emb_inputs\" : (field_aware_embedding, [\"userId\", \"movieId\"])\n",
    "}\n",
    "inputs_wrapper = trs.inputs.InputsWrapper(schema)\n",
    "\n",
    "# initialize field-aware factorizatiob machine model\n",
    "ffm = trs.models.FieldAwareFactorizationMachineModel(embed_size, num_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(ffm, trs.models._Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dict(\n",
    "    userId  = torch.randint(100, size=(16, 1)), \n",
    "    movieId = torch.randint(100, size=(16, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.0732e-01],\n",
      "        [ 2.4323e-01],\n",
      "        [-7.9130e-01],\n",
      "        [-4.9572e-01],\n",
      "        [-8.0709e-01],\n",
      "        [ 1.9575e-01],\n",
      "        [-9.8850e-01],\n",
      "        [ 3.2924e-01],\n",
      "        [-2.6344e-01],\n",
      "        [ 2.4496e+00],\n",
      "        [-1.2063e+00],\n",
      "        [ 5.6532e-01],\n",
      "        [-3.0166e-04],\n",
      "        [-5.1893e-01],\n",
      "        [-1.2735e-01],\n",
      "        [ 1.0425e+00]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "embed_inputs    = inputs_wrapper(inputs)\n",
    "\n",
    "# prediction by model\n",
    "output = ffm(**embed_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 8\n",
    "n = 5\n",
    "b = 16\n",
    "\n",
    "feat_inputs = torch.randn(size=(b, n, 1))\n",
    "field_emb = torch.randn(size=(b, n, e))\n",
    "field_aware_emb = torch.randn(size=(b, n * n, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_emb = torch.randn(size=(b, 2, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afm_model = torch.jit.trace(trs.models.AttentionalFactorizationMachineModel(e, n, 8, 0.0), (feat_inputs, field_emb))\n",
    "afm_model(feat_inputs, field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_model = torch.jit.trace(trs.models.DeepAndCrossNetworkModel(e * n, 1, [10], 2, 1, [0.0], nn.ReLU6()), field_emb)\n",
    "dc_model(field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffm_model = torch.jit.trace(trs.models.DeepFieldAwareFactorizationMachineModel(e, n, 1, [10], 0.0, [0.0], nn.Tanh(), 1), field_aware_emb)\n",
    "dffm_model(field_aware_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_model = torch.jit.trace(trs.models.DeepFactorizationMachineModel(e, n, [10], 0.0, [0.0], nn.ReLU(), 1), (feat_inputs, field_emb))\n",
    "dfm_model(feat_inputs, field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = torch.jit.trace(trs.models.FactorizationMachineModel(e, n, 0.1), (feat_inputs, field_emb))\n",
    "fm(feat_inputs, field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnn = torch.jit.trace(trs.models.FactorizationMachineSupportedNeuralNetwork(e, n, 1, [10], 0.1, [0.1], nn.Tanh()), (feat_inputs, field_emb))\n",
    "fmnn(feat_inputs, field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatdffm = torch.jit.trace(trs.models.FieldAttentiveDeepFieldAwareFactorizationMachineModel(e, n, 10, [10], 16, 0.0, [0.1], nn.Tanh()), field_aware_emb)\n",
    "fatdffm(field_aware_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm = torch.jit.trace(trs.models.FieldAwareFactorizationMachineModel(e, n), (feat_inputs, field_aware_emb))\n",
    "ffm(feat_inputs, field_aware_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = torch.jit.trace(trs.models.LogisticRegressionModel(n), feat_inputs)\n",
    "lr(feat_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf = torch.jit.trace(trs.models.NeuralCollaborativeFilteringModel(e, 1, [10], [0.1], nn.ReLU6()), user_item_emb)\n",
    "ncf(user_item_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afm = trs.layers.AttentionalFactorizationMachineLayer(e, n, attn_size=8, dropout_p=0.0)\n",
    "print(afm(field_emb)[0].size())\n",
    "print(afm(field_emb)[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blin = trs.layers.BilinearNetworkLayer(e, n, 1, 3)\n",
    "blin(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cen = trs.layers.ComposeExcitationNetworkLayer(n)\n",
    "cen(field_aware_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cin = trs.layers.CompressInteractionNetworkLayer(e, n, 1, [10, 10, 10])\n",
    "cin(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = trs.layers.CrossNetworkLayer(5, e, n)\n",
    "cross(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = trs.layers.FactorizationMachineLayer(0.0)\n",
    "fm(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm = trs.layers.FieldAwareFactorizationMachineLayer(n, 0.0)\n",
    "ffm(field_aware_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = trs.layers.InnerProductNetworkLayer(n)\n",
    "inner(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = trs.layers.MultilayerPerceptronLayer(1, [10, 10, 10], embed_size=e, num_fields=n, dropout_p=[0.0, 0.0, 0.0])\n",
    "mlp(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = trs.layers.OuterProductNetworkLayer(e, n)\n",
    "outer(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = trs.layers.WideLayer(e, n, 1)\n",
    "wide(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
