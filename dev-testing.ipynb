{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torecsys as trs\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-da9b2037176a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'name'"
     ]
    }
   ],
   "source": [
    "x = logging.Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"./log_dir\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get samples data from movielens as a example\n",
    "# _, movies_df, ratings_df, _ = trs.data.sampledata.load_ml_data(size=\"latest-small\")\n",
    "# movies_df[\"year\"] = movies_df.title.apply(lambda x: re.findall(r\"\\((\\d+)\\)\", x))\n",
    "# movies_df[\"year\"] = movies_df.year.apply(lambda x: int(x[0]) if len(x) > 0 else np.nan)\n",
    "# movies_df = pd.concat([\n",
    "#     movies_df, \n",
    "#     pd.get_dummies(movies_df.genres.apply(\n",
    "#         lambda x: x.split(\"|\")).apply(pd.Series).stack()).sum(level=0)\n",
    "# ], axis=1).drop([\"title\", \"genres\"], axis=1)\n",
    "# merged = pd.merge(ratings_df, movies_df, on=\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = torch.jit.trace(trs.inputs.base.EmbeddingDict([\"userId\", \"itemId\"], [\"single_index\", \"single_index\"], [100, 50], [16, 16]), {\"userId\": (torch.Tensor([[1]]).long()), \"itemId\": (torch.Tensor([[1]]).long())})\n",
    "emb_dict({\"userId\": (torch.Tensor([[1]]).long()), \"itemId\": (torch.Tensor([[1]]).long())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_emb = torch.jit.trace(trs.inputs.base.FieldAwareMultipleIndexEmbedding(16, [100, 10]), torch.Tensor([[1, 1]]).long())\n",
    "fa_emb(torch.Tensor([[1, 1]]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_emb = torch.jit.trace(trs.inputs.base.ListIndexEmbedding(16, 100, num_heads=4), torch.Tensor([[1, 4, 5]]).long())\n",
    "list_emb(torch.Tensor([[1, 4, 5]]).long())\n",
    "# try to fix if use trace\n",
    "# list_emb.show_attention(torch.Tensor([[1, 4, 5]]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_emb = torch.jit.trace(trs.inputs.base.MultipleIndexEmbedding(16, [100, 10]), torch.Tensor([[1, 1]]).long())\n",
    "mlp_emb(torch.Tensor([[1, 1]]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_emb = torch.jit.trace(trs.inputs.base.SequenceIndexEmbedding(16, 100), (torch.Tensor([[3, 1, 0], [4, 2, 5]]).long(), torch.Tensor([2, 3]).long()))\n",
    "sq_emb(torch.Tensor([[3, 1, 0], [4, 2, 5]]).long(), torch.Tensor([2, 3]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_emb = torch.jit.trace(trs.inputs.base.SingleIndexEmbedding(16, 100), torch.Tensor([[1]]).long())\n",
    "index_emb(torch.Tensor([[1]]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = torch.jit.trace(trs.inputs.base.StackedInputs([\"userId\", \"itemId\"], [\"single_index\", \"single_index\"], [100, 50], [16, 16]), ({\"userId\": (torch.Tensor([[1]]).long()), \"itemId\": (torch.Tensor([[1]]).long())}))\n",
    "stack({\"userId\": (torch.Tensor([[1]]).long()), \"itemId\": (torch.Tensor([[1]]).long())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = torch.jit.trace(trs.inputs.base.ValueInputs(5), torch.randn(size=(1, 5)))\n",
    "val(torch.randn(size=(1, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_inputs = torch.jit.trace(trs.inputs.base.ImageInputs(16, 3, [8, 8], [1, 1], [1, 1], [1, 1]), torch.randn(4, 3, 64, 64))\n",
    "img_inputs(torch.randn(4, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_img = torch.jit.trace(trs.inputs.base.PretrainedImageInputs(16, \"resnet18\"), torch.randn(4, 3, 64, 64))\n",
    "pre_img(torch.randn(4, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"sequenceId\"     : torch.randint(100, size=(8, 4)),\n",
    "    \"sequenceLength\" : torch.Tensor([4] * 8),\n",
    "    \"values\"         : torch.randn(size=(8, 8)),\n",
    "    \"userId\"         : torch.randint(100, size=(8, 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_schema = [\n",
    "    (trs.inputs.base.SequenceIndexEmbedding(8, 100), [\"sequenceId\"], [\"sequenceLength\"]),\n",
    "    (trs.inputs.base.ValueInputs(8), [\"values\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_schema = [\n",
    "    (trs.inputs.base.SingleIndexEmbedding(16, 100), [\"userId\"]), \n",
    "    (trs.inputs.base.ConcatInputs(cat_schema), [\"sequenceId\", \"sequenceLength\", \"values\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = trs.inputs.base.StackedInputs(stacked_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_schema = {\n",
    "    \"stacked\": (trs.inputs.base.StackedInputs(stacked_schema), [\"userId\", \"sequenceId\", \"sequenceLength\", \"values\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped = trs.inputs.InputsWrapper(wrapped_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stacked': tensor([[[-0.8068,  1.8913,  0.9308, -0.2586, -0.8677,  0.8212,  0.5583,\n",
       "           -0.4547,  0.7802, -0.2594,  0.4328, -0.5197, -0.1188,  0.6041,\n",
       "           -0.1233,  0.6427],\n",
       "          [ 0.0865,  0.0638, -0.1338, -0.2038,  0.1361,  0.2313,  0.0161,\n",
       "            0.1685, -0.8561,  0.6156,  1.0580, -0.1640,  2.1856, -0.0967,\n",
       "           -0.4956,  0.5661]],\n",
       " \n",
       "         [[ 0.0071, -0.2882, -1.2425, -0.1356,  0.6158, -2.0084, -1.1312,\n",
       "           -0.0764, -0.4758, -0.0837,  0.6379, -1.7833, -0.7717, -1.1022,\n",
       "           -0.6417, -0.5653],\n",
       "          [ 0.1331,  0.1828, -0.0099, -0.3414,  0.1036,  0.2049, -0.0202,\n",
       "            0.1084, -1.7258,  1.1650, -0.5486,  0.2988,  0.3021, -1.3602,\n",
       "           -0.6615,  0.7942]],\n",
       " \n",
       "         [[ 1.6530, -1.1351, -0.1689,  0.6209,  0.7906,  1.5958,  0.6635,\n",
       "            1.3527,  0.5280, -1.3512,  1.3713, -0.7543,  1.9354, -0.7788,\n",
       "           -0.8546,  0.1996],\n",
       "          [ 0.1855,  0.2000, -0.0851, -0.2682,  0.1109,  0.1906, -0.0362,\n",
       "            0.2185, -0.1249, -0.3391, -0.6115, -0.7631,  0.5525,  1.5960,\n",
       "            2.1302, -0.9355]],\n",
       " \n",
       "         [[ 0.0331,  0.9302, -0.0828, -1.3417,  0.3530, -0.5190,  3.3029,\n",
       "           -0.1178, -0.6763,  0.9616,  0.6375,  0.9671, -0.0302,  0.9272,\n",
       "            1.0633, -0.0239],\n",
       "          [ 0.1582, -0.0243, -0.0854, -0.1377,  0.0253,  0.2951,  0.0393,\n",
       "            0.0864,  0.7881, -0.6230, -0.5125, -0.7455,  0.4958, -1.0388,\n",
       "           -1.0218,  0.6136]],\n",
       " \n",
       "         [[-0.2036, -0.0529,  0.1736,  0.8144, -1.3636,  0.7873,  0.6191,\n",
       "           -1.3087,  0.8804,  1.5518, -0.6558,  1.2211, -0.1705,  0.6382,\n",
       "           -1.1868,  0.6122],\n",
       "          [ 0.1855,  0.0761, -0.2068, -0.2014,  0.1059,  0.1427, -0.0439,\n",
       "            0.1349, -0.2287,  0.4151, -2.4412, -0.6370, -1.7079,  1.1475,\n",
       "           -1.3291,  1.4290]],\n",
       " \n",
       "         [[-0.8541,  1.5021, -0.0548,  1.8424,  0.1668, -0.0148,  0.5183,\n",
       "            1.0339,  0.0875,  0.5913,  0.3345, -3.5645,  0.4670, -0.9019,\n",
       "           -0.6090,  1.1253],\n",
       "          [ 0.1635,  0.1107, -0.1300, -0.2898,  0.1606,  0.2634, -0.0782,\n",
       "            0.1748,  0.3117,  0.6828,  0.3433,  1.9894, -0.2381,  0.7935,\n",
       "            0.1018, -1.1442]],\n",
       " \n",
       "         [[ 0.8609,  0.8907, -1.1468,  0.2448, -0.8818,  1.5078,  0.3571,\n",
       "            0.7214, -0.3253, -0.1901, -0.6817, -0.7802,  0.6320, -0.0908,\n",
       "            0.6325, -0.0180],\n",
       "          [ 0.0624,  0.1463, -0.0883, -0.2397,  0.0462,  0.0906, -0.1132,\n",
       "            0.0418, -0.7996, -2.6209, -0.7283, -0.3608, -0.5356, -1.4745,\n",
       "           -0.1247, -0.1656]],\n",
       " \n",
       "         [[-0.6212, -0.3779, -1.4590, -1.8081, -1.9057, -0.9392, -1.0105,\n",
       "            0.4502, -0.1247, -1.2792, -0.1218, -0.5136, -1.1621,  0.4577,\n",
       "            1.4397, -0.0968],\n",
       "          [ 0.0681,  0.1612,  0.0323, -0.2308, -0.0155,  0.0244, -0.0814,\n",
       "           -0.0339,  0.1276, -1.3043,  0.1412,  1.1797,  0.2635, -0.2689,\n",
       "           -0.7515,  0.5931]]], grad_fn=<CatBackward>)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Field-aware Factorization Machine with trs.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 16\n",
    "num_fields = 2\n",
    "\n",
    "userIdSize = 100\n",
    "itemIdSize = 100\n",
    "\n",
    "# inititalize embedding fields\n",
    "feat_inputs_embedding = trs.inputs.base.MultipleIndexEmbedding(\n",
    "    1, [userIdSize, itemIdSize]\n",
    ")\n",
    "field_aware_embedding = trs.inputs.base.FieldAwareMultipleIndexEmbedding(\n",
    "    embed_size, [itemIdSize, itemIdSize]\n",
    ")\n",
    "\n",
    "# define schema of wrapper and initialize InputsWrapper\n",
    "schema = {\n",
    "    \"feat_inputs\"      : (feat_inputs_embedding, [\"userId\", \"movieId\"]),\n",
    "    \"field_emb_inputs\" : (field_aware_embedding, [\"userId\", \"movieId\"])\n",
    "}\n",
    "inputs_wrapper = trs.inputs.InputsWrapper(schema)\n",
    "\n",
    "# initialize field-aware factorizatiob machine model\n",
    "ffm = trs.models.FieldAwareFactorizationMachineModel(embed_size, num_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(ffm, trs.models._Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dict(\n",
    "    userId  = torch.randint(100, size=(16, 1)), \n",
    "    movieId = torch.randint(100, size=(16, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.0732e-01],\n",
      "        [ 2.4323e-01],\n",
      "        [-7.9130e-01],\n",
      "        [-4.9572e-01],\n",
      "        [-8.0709e-01],\n",
      "        [ 1.9575e-01],\n",
      "        [-9.8850e-01],\n",
      "        [ 3.2924e-01],\n",
      "        [-2.6344e-01],\n",
      "        [ 2.4496e+00],\n",
      "        [-1.2063e+00],\n",
      "        [ 5.6532e-01],\n",
      "        [-3.0166e-04],\n",
      "        [-5.1893e-01],\n",
      "        [-1.2735e-01],\n",
      "        [ 1.0425e+00]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "embed_inputs    = inputs_wrapper(inputs)\n",
    "\n",
    "# prediction by model\n",
    "output = ffm(**embed_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 8\n",
    "n = 5\n",
    "b = 16\n",
    "\n",
    "feat_inputs = torch.randn(size=(b, n, 1))\n",
    "field_emb = torch.randn(size=(b, n, e))\n",
    "field_aware_emb = torch.randn(size=(b, n * n, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_emb = torch.randn(size=(b, 2, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afm_model = torch.jit.trace(trs.models.AttentionalFactorizationMachineModel(e, n, 8, 0.0), (feat_inputs, field_emb))\n",
    "afm_model(feat_inputs, field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_model = torch.jit.trace(trs.models.DeepAndCrossNetworkModel(e * n, 1, [10], 2, 1, [0.0], nn.ReLU6()), field_emb)\n",
    "dc_model(field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffm_model = torch.jit.trace(trs.models.DeepFieldAwareFactorizationMachineModel(e, n, 1, [10], 0.0, [0.0], nn.Tanh(), 1), field_aware_emb)\n",
    "dffm_model(field_aware_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_model = torch.jit.trace(trs.models.DeepFactorizationMachineModel(e, n, [10], 0.0, [0.0], nn.ReLU(), 1), (feat_inputs, field_emb))\n",
    "dfm_model(feat_inputs, field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = torch.jit.trace(trs.models.FactorizationMachineModel(e, n, 0.1), (feat_inputs, field_emb))\n",
    "fm(feat_inputs, field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnn = torch.jit.trace(trs.models.FactorizationMachineSupportedNeuralNetwork(e, n, 1, [10], 0.1, [0.1], nn.Tanh()), (feat_inputs, field_emb))\n",
    "fmnn(feat_inputs, field_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatdffm = torch.jit.trace(trs.models.FieldAttentiveDeepFieldAwareFactorizationMachineModel(e, n, 10, [10], 16, 0.0, [0.1], nn.Tanh()), field_aware_emb)\n",
    "fatdffm(field_aware_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm = torch.jit.trace(trs.models.FieldAwareFactorizationMachineModel(e, n), (feat_inputs, field_aware_emb))\n",
    "ffm(feat_inputs, field_aware_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = torch.jit.trace(trs.models.LogisticRegressionModel(n), feat_inputs)\n",
    "lr(feat_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf = torch.jit.trace(trs.models.NeuralCollaborativeFilteringModel(e, 1, [10], [0.1], nn.ReLU6()), user_item_emb)\n",
    "ncf(user_item_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afm = trs.layers.AttentionalFactorizationMachineLayer(e, n, attn_size=8, dropout_p=0.0)\n",
    "print(afm(field_emb)[0].size())\n",
    "print(afm(field_emb)[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blin = trs.layers.BilinearNetworkLayer(e, n, 1, 3)\n",
    "blin(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cen = trs.layers.ComposeExcitationNetworkLayer(n)\n",
    "cen(field_aware_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cin = trs.layers.CompressInteractionNetworkLayer(e, n, 1, [10, 10, 10])\n",
    "cin(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = trs.layers.CrossNetworkLayer(5, e, n)\n",
    "cross(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = trs.layers.FactorizationMachineLayer(0.0)\n",
    "fm(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm = trs.layers.FieldAwareFactorizationMachineLayer(n, 0.0)\n",
    "ffm(field_aware_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = trs.layers.InnerProductNetworkLayer(n)\n",
    "inner(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = trs.layers.MultilayerPerceptronLayer(1, [10, 10, 10], embed_size=e, num_fields=n, dropout_p=[0.0, 0.0, 0.0])\n",
    "mlp(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = trs.layers.OuterProductNetworkLayer(e, n)\n",
    "outer(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = trs.layers.WideLayer(e, n, 1)\n",
    "wide(field_emb).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
