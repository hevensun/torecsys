import torch
import torch.nn.functional as F

# change the loss's inputs from embedding vectors to the dot-product of vectors
# with shape = (batch size, 1) and (batch size, num_neg)
def skip_gram_loss(cout: torch.Tensor, pout: torch.Tensor, nout: torch.Tensor) -> torch.Tensor:
    r"""loss function used in SkipGram, i.e. one of the most famous and traditional algorithm
    in Word2Vec, which is calculated by the following formula:
    :math:`loss = - \sum_{c=1}^{C} u_{j_{c}^{*}} + C log ( \sum_{j'=1}^{v} e^{u_{j'}} )` .
    
    Args:
        cout (torch.Tensor), shape = (batch size, 1, embed size), dtype = torch.float: content or anchor to be learnt with samples
        pout (torch.Tensor), shape = (batch size, 1, embed size), dtype = torch.float: positive samples from dataset
        nout (torch.Tensor), shape = (batch size, number of negative samples, embed size), dtype = torch.float: negative samples generated by negative sampler
    
    Returns:
        torch.Tensor, shape = (1, ), dtype = torch.float: loss of skipgram algorithm
    """
    # positive values' part, return shape = (batch size, )
    pval = F.logsigmoid((cout * pout).sum(dim=2)).squeeze()

    # negative values' part, bmm (batch size, num neg, embed size) 
    # by (batch size, embed size, 1). Hence, shape = (batch size, num nge, 1)
    nval = torch.bmm(neg_inputs, content_inputs.transpose(1, 2))

    # sum by second dimension, and hence return shape = (batch size, 1, 1)
    # take logsigmoid and squeeze, then return shape = (batch size, )
    nval = F.logsigmoid(- nval.sum(dim=1)).squeeze()

    # calculate loss and take aggregation
    loss = - (pval + nval).mean()

    return loss

# to be developed
def cbow_loss():
    pass

